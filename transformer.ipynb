{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from model import TransformerModel, SentimentGRUWithGlove\n",
    "from dataset import SentimentAnalysisDataset\n",
    "from test import run_test\n",
    "\n",
    "import numpy as np\n",
    "from dataset import LABEL_MAP\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size=100, embedding_dim=200) -> None:\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=5, batch_first=True)\n",
    "        self.tranformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.33),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50,3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tranformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = TransformerModel(input_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading datasets\n",
      "-> Loading word embeddings\n",
      "-> Loading word embeddings\n",
      "-> Initalizing model\n",
      "Using model SentimentGRUWithGlove\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZERS = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adadelta': torch.optim.Adadelta,\n",
    "    'sgd': torch.optim.SGD,\n",
    "    'adagrad': torch.optim.Adagrad\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'data_path': 'data',\n",
    "    'model_name': 'transformer_classifier_A',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 30,\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-3,\n",
    "    'print_steps': 1000\n",
    "}\n",
    "\n",
    "data_path = args['data_path']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('-> Loading datasets')\n",
    "train_dataset = SentimentAnalysisDataset(os.path.join(data_path, 'trainEmotions.csv'))\n",
    "train_size = len(train_dataset)\n",
    "\n",
    "test_dataset = SentimentAnalysisDataset(os.path.join(data_path, 'testEmotions.csv'))\n",
    "test_size = len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Initalizing model\n",
      "Using model TransformerModel\n"
     ]
    }
   ],
   "source": [
    "print('-> Initalizing model')\n",
    "model = TransformerModel(input_size=100)\n",
    "model.to(device)\n",
    "model = model.float()\n",
    "print(f'Using model {model.__class__.__name__}')\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = OPTIMIZERS[args['optimizer']](model.parameters(), lr=args['lr'])\n",
    "\n",
    "t = datetime.now().strftime('%m_%d_%H_%M')\n",
    "\n",
    "results = []\n",
    "max_test_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, dataset, loss_func, device):\n",
    "\n",
    "    test_loss = 0.0\n",
    "    y_true = []\n",
    "    y_predict = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tokens, label in dataset:\n",
    "            \n",
    "            tokens = tokens.to(device).float()\n",
    "            label = label.to(device)\n",
    "\n",
    "            y_true.append(int(label.argmax()))\n",
    "\n",
    "            if tokens.squeeze().dim() == 0 or len(tokens.squeeze()) == 0:\n",
    "                # Predict neutral if no token after processing \n",
    "                # e.g., only stopwords in the original text\n",
    "                y_predict.append(int(LABEL_MAP['neutral']))\n",
    "                continue\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(tokens)\n",
    "            y_predict.append(int(torch.softmax(output, dim=0).argmax()))\n",
    "            L = loss_func(output.view(1,-1), label.view(1,-1))\n",
    "            test_loss += L.item()\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_predict = np.array(y_predict)\n",
    "\n",
    "    return accuracy_score(y_true, y_predict), confusion_matrix(y_true, y_predict), test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Running test\n",
      "Accuracy: 0.37399214389084146\n",
      "Loss: 5324.232888102531\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "print('-> Running test')\n",
    "test_acc, _, test_loss = run_test(model, test_dataset, loss, device)\n",
    "print('Accuracy:', test_acc)\n",
    "print('Loss:', test_loss)\n",
    "print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "---------------------------\n",
      "L: 1.332444  [0/14504]\n",
      "L: 1.102450  [1000/14504]\n",
      "L: 1.098958  [2000/14504]\n",
      "L: 1.096642  [3000/14504]\n",
      "L: 1.095479  [4000/14504]\n",
      "L: 1.095589  [5000/14504]\n",
      "L: 1.093494  [6000/14504]\n",
      "L: 1.092190  [7000/14504]\n",
      "L: 1.091564  [8000/14504]\n",
      "L: 1.089348  [9000/14504]\n",
      "L: 1.088868  [10000/14504]\n",
      "L: 1.086469  [11000/14504]\n",
      "L: 1.085148  [12000/14504]\n",
      "L: 1.083864  [13000/14504]\n",
      "L: 1.080710  [14000/14504]\n",
      "Epoch Loss: 1.0808698246060475\n",
      "------------------\n",
      "-> Running test\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss \u001b[38;5;241m/\u001b[39m train_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-> Running test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m test_acc, _, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_loss)\n",
      "File \u001b[0;32m~/Projects/sentiment_analysis/test.py:28\u001b[0m, in \u001b[0;36mrun_test\u001b[0;34m(model, dataset, loss_func, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m model(tokens)\n\u001b[1;32m     27\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m y_predict\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m L \u001b[38;5;241m=\u001b[39m loss_func(y_prob\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     30\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    print(f\"Epoch {epoch+1}/{args['epochs']}\\n---------------------------\")\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, (tokens, label) in enumerate(train_dataset):\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            output = model(tokens)\n",
    "            L = loss(output.view(1,-1), label.view(1,-1))\n",
    "            train_loss += L.item()\n",
    "            L.backward()\n",
    "        except Exception as e:\n",
    "            print(i,tokens, label)\n",
    "            print(e)\n",
    "\n",
    "        if i & args['batch_size'] == 0:\n",
    "            # Batched Backpropagation\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if i % args['print_steps'] == 0:\n",
    "            print(f'L: {train_loss / (i+1):>7f}  [{i}/{train_size}]')\n",
    "\n",
    "    print(f'Epoch Loss: {train_loss / train_size}\\n------------------')\n",
    "\n",
    "    print('-> Running test')\n",
    "    test_acc, _, test_loss = run_test(model, test_dataset, loss, device)\n",
    "    print('Accuracy:', test_acc)\n",
    "    print('Loss:', test_loss)\n",
    "    print('---------------')\n",
    "\n",
    "    if test_acc > max_test_acc:    \n",
    "        print('-> Saving state')\n",
    "        torch.save(model.state_dict(), f'{args[\"model_name\"]}_{t}.state')\n",
    "\n",
    "    results.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss / train_size,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving results CSV')\n",
    "with open(f'results_{args['model_name']}_{t}.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=list(results[0].keys()))\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        writer.writerow(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d538358a4a844c2698117f72ddbeea822b864d784b5f9260aa8445549be4a00"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
